Abstract for our project

There are now novel sources of high-quality data for tackling a broad array of pressing scientific and engineering problems that need to be solved to improve our quality of life. However, such high-fidelity data often comes from costly simulations, limiting the available data. Thus, developing cost-efficient sampling methods combined with rigorous, data-driven error measures for the resulting models is critically important.  This project combines ideas from computational mathematics and statistics to discover these cost-efficient and confident sampling methods.  Students involved in this project will be educated to become the next generation of science-based computational researchers who can adeptly work in diverse and multi-disciplinary scientific teams pushing forward the frontiers of scientific knowledge.  

Our framework features methodologies (with supporting theory and algorithms) that extend classical low discrepancy (i.e., highly stratified) sampling techniques for a broad range of challenging scenarios encountered in modern scientific problems, including cost-efficient Bayesian inference, efficient subsampling of massive data, multi-fidelity modeling, and density estimation. Major emphasis is placed on demonstrating the effectiveness of these methods for accelerating scientific discoveries, especially for the PIs’ ongoing collaborations on the study of heavy-ion collisions and real-time engine control of unmanned aircraft vehicles, but also for new collaborations that will be developed over the project. Our efforts will catalyze closer collaborations between the scientific and data science communities by broadening the application of low-discrepancy sampling for the complex settings featured in modern scientific and engineering problems. Such collaborations will be further strengthened via our open-source Python QMC library QMCPy.


=========================================================


A two-paragraph abstract in plain text (no special symbols) for one of you. Quoting from NSF documents,
 
“Abstracts are a public record of active and expired awards and are an important source of information on NSF activities. The purpose of the Abstract is to describe the project and justify the expenditure of Federal funds. Abstracts must not contain inappropriate or confidential information, and because they are available to such a wide audience, high standards of quality must be maintained in preparing them.

The NSF award abstract has two parts, which should appear in the following order:

  -Part 1: A nontechnical description of the project, which explains the project's significance and importance. This description also serves as a public justification for NSF funding by articulating how the project serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity, and welfare; or to secure the national defense. This part of the abstract should describe the fundamental issues the project seeks to address, as well as other potential benefits, such as how the project advances the field, supports education and diversity, or benefits society. This part should be understandable by a broad audience.

  -Part 2: A technical description of the project that states its goals and scope, the methods and approaches to be used, and its potential contribution. In many cases, the technical project description may be a modified version of the project summary that is submitted with the proposal. However, the technical description should reflect any changes in the project's goals made after the review process.
 
Upon award of a proposal, the Abstract is available in the Award Search application.”

=========================================================
Project Summary

Overview
With breakthroughs in experimental methods and computational technology, there are now novel sources of high-quality data for tackling a broad array of pressing problems in science and engineer- ing. However, the generation of such high-fidelity data often requires costly experiments and/or simulations, which can significantly limit the amount of data available for scientific investigation. Given this cost bottleneck, it is of critical importance to develop cost-efficient sampling methods for data generation and model training. Furthermore, for scientific inference, such sampling meth- ods need to be performed with confidence; they need to be coupled with theoretically sound and data-driven stopping rules, which guarantee the resulting statistical model achieves a desired er- ror tolerance. This is paramount for reliable scientific discovery: it provides a quantification of uncertainty for scientific inference, thus protecting against spurious findings.
This project will develop a novel and timely suite of methods that jointly addresses this cru- cial need for cost-efficient and confident sampling for scientific discovery. Our framework features methodologies (with supporting theory and algorithms) that extend classical low discrepancy (i.e., highly stratified) sampling techniques for a broad range of challenging scenarios encountered in mod- ern scientific problems, including cost-efficient Bayesian inference, efficient subsampling of massive data, multi-fidelity modeling, and density estimation. Major emphasis is placed on demonstrating the effectiveness of these methods for accelerating scientific discoveries, especially for the PIs’ on- going collaborations on the study of heavy-ion collisions and real-time engine control of unmanned aircraft vehicles, but also for new collaborations that will be developed over the project.
Intellectual Merit
Our project investigates four novel directions that extend low discrepancy sampling to complex set- tings in modern scientific and engineering problems. Each direction is necessitated by a motivating scientific problem from the PIs’ multi-disciplinary collaborations and plays an integral role in our proposed suite of methods for accelerating scientific discovery. The first, called cost-efficient Stein points, extends low-discrepancy sampling for expensive Bayesian computation problems, where each posterior evaluation requires a forward run of a costly scientific simulator. The second di- rection explores adaptive algorithms and stopping rules for confident multifidelity sampling, which is widely used in the physical sciences. The third extends low discrepancy sampling for efficient and confident big data analysis to facilitate real-time decision-making. The last direction investi- gates low discrepancy sampling for distribution, density, and quantile estimation. Each direction will involve the development of novel methodology, theory, and algorithms, with a keen focus on addressing scientific needs in our aforementioned ongoing collaborations.
Broader Impacts
The proposed suite of methods paves the way for transformative scientific research, equipping practitioners with cost-efficient and confident sampling methods (with supporting theory and al- gorithms) for accelerating scientific and engineering discoveries. Our project will catalyze closer collaborations between the scientific and data science communities by broadening the application of low-discrepancy sampling for the complex settings featured in modern scientific and engineer- ing problems. Such collaborations will be further strengthened via our open-source Python QMC library QMCPy, which provides an accessible and well-documented software connecting state-of- the-art low-discrepancy methods with the broader scientific community. This project will also train the next generation of science-based computational researchers, who can adeptly work in diverse and multi-disciplinary scientific teams pushing forward the frontiers of scientific knowledge.

